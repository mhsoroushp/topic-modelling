{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arik_\\Documents\\Dokumente\\Job_Clausthal\\TNTM\\TNTM_Revision_TNNLS\\TNTM\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\arik_\\anaconda3\\envs\\tlda\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\arik_\\anaconda3\\envs\\tlda\\lib\\site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\arik_\\anaconda3\\envs\\tlda\\lib\\site-packages (from gensim) (1.7.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\arik_\\anaconda3\\envs\\tlda\\lib\\site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\arik_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arik_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cpu\n",
      "current device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "from Code.Evaluate.Metrics import score_all, get_tw_embeddings\n",
    "from Code.TNTM.TNTM_bow import TNTM_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#with open(\"C:\\\\Users\\\\arik_\\\\\\Documents\\\\Dokumente\\\\Job_Clausthal\\\\TNTM\\\\TNTM_Revision_TNNLS\\\\TNTM\\\\Data\\\\DataOctis2\\\\octis_dataset_20ng.pickle\", \"rb\") as file:\n",
    "#    bow_data = pickle.load(file)\n",
    "\n",
    "with open(\"C:\\\\Users\\\\arik_\\\\Documents\\\\Dokumente\\\\Job_Clausthal\\\\TNTM\\\\TNTM_Revision_TNNLS\\\\TNTM\\Data\\DataOctis2\\\\octis_dataset_20ng.pickle\", \"rb\") as file:\n",
    "    octis_dataset = pickle.load(file)\n",
    "\n",
    "corpus = octis_dataset.get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3350/3350 [00:32<00:00, 104.31it/s]\n",
      "100%|██████████| 48018/48018 [10:08<00:00, 78.94it/s] \n"
     ]
    }
   ],
   "source": [
    "tw_emb = get_tw_embeddings(octis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:\\\\Users\\\\arik_\\\\Documents\\\\Dokumente\\\\Job_Clausthal\\\\TNTM\\\\TNTM_Revision_TNNLS\\\\TNTM\\Data\\DataOctis\\\\cleaned_embedding_df_20ng_BERT.pickle\", 'rb') as f:\n",
    "    embedding_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.sort_values(by = \"word\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_ten_lis = []\n",
    "\n",
    "for i in range(len(embedding_df)):\n",
    "    embedding_ten_lis.append(embedding_df[\"embedding\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.sort_values(by = \"word\", inplace = True)\n",
    "embedding_ten_lis = []\n",
    "\n",
    "embedded_words = embedding_df.index.tolist()\n",
    "\n",
    "for i in range(len(embedding_df)):\n",
    "    embedding_ten_lis.append(embedding_df[\"embedding\"].iloc[i])\n",
    "embedding_ten = torch.stack(embedding_ten_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = embedded_words \n",
    "vocab_set = set(vocab)\n",
    "corpus = [[word for word in doc if word in vocab_set] for doc in octis_dataset.get_corpus()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TNTM_bow(\n",
    "    n_topics = 10,\n",
    "    save_path = \"C:\\\\Users\\\\arik_\\\\Documents\\\\Dokumente\\\\Job_Clausthal\\\\TNTM\\\\TNTM_Revision_TNNLS\\\\TNTM\\\\msc\\\\SavedResults\\\\model_v1.pth\",\n",
    "    n_dims = 11,\n",
    "    n_hidden_units = 200,\n",
    "    n_encoder_layers = 3,\n",
    "    enc_lr = 1e-4,\n",
    "    dec_lr = 1e-3,\n",
    "    n_epochs = 0,\n",
    "    #batch_size = 128,\n",
    "    batch_size = 256,\n",
    "    dropout_rate_encoder = 0.3,\n",
    "    prior_variance =  0.995, \n",
    "    prior_mean = None,\n",
    "    n_topwords = 200,\n",
    "    device = None, \n",
    "    validation_set_size = 0.2, \n",
    "    early_stopping = True,\n",
    "    n_epochs_early_stopping = 10,\n",
    "    return_embeddings = False,\n",
    "    eps = 1e-4,\n",
    "    umap_hyperparams = {'n_neighbors': 15, 'min_dist': 0.0},\n",
    "    return_losses= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18846it [04:40, 67.22it/s] \n",
      "c:\\Users\\arik_\\Documents\\Dokumente\\Job_Clausthal\\TNTM\\TNTM_Revision_TNNLS\\TNTM\\Code\\TNTM\\TNTM_bow.py:170: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mus_init_ten = torch.tensor(mus_init).to(self.device)\n",
      "c:\\Users\\arik_\\Documents\\Dokumente\\Job_Clausthal\\TNTM\\TNTM_Revision_TNNLS\\TNTM\\Code\\TNTM\\TNTM_bow.py:171: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  L_lower_init_ten = torch.tensor(L_lower_init).to(self.device)\n",
      "c:\\Users\\arik_\\Documents\\Dokumente\\Job_Clausthal\\TNTM\\TNTM_Revision_TNNLS\\TNTM\\Code\\TNTM\\TNTM_bow.py:172: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_diag_init_ten = torch.tensor(log_diag_init).to(self.device)\n"
     ]
    }
   ],
   "source": [
    "res_all = model.fit(\n",
    "    corpus = corpus, \n",
    "    vocab = vocab, \n",
    "    embeddings = embedding_ten\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all documents shorter than 10 words\n",
    "corpus = [doc for doc in corpus if len(doc) >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "octis_dataset._Dataset__corpus = corpus\n",
    "octis_dataset._Dataset__vocab = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 612.20it/s]\n",
      "100%|██████████| 10/10 [00:00<?, ?it/s]\n",
      "100%|██████████| 5/5 [00:30<00:00,  6.06s/it]\n"
     ]
    }
   ],
   "source": [
    "evaluation_result = score_all(\n",
    "    dataset = octis_dataset,\n",
    "    tw_emb=tw_emb,\n",
    "    n_words=10,\n",
    "    result = {'topics': res_all[0], \n",
    "              \"topic-word-matrix\": res_all[1]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['tek', 'wustl', 'chem', ..., 'funny', 'german', 'less'],\n",
       "        ['useless', 'simple', 'ideal', ..., 'domain', 'doc', 'zuma'],\n",
       "        ['resident', 'party', 'leader', ..., 'locate', 'mailer', 'zuma'],\n",
       "        ...,\n",
       "        ['draft', 'pool', 'controller', ..., 'mark', 'mc', 'zuma'],\n",
       "        ['simon', 'jay', 'jonathan', ..., 'fourth', 'france', 'zuma'],\n",
       "        ['schedule', 'usage', 'address', ..., 'lewis', 'leo', 'zuma']],\n",
       "       dtype='<U14'),\n",
       " array([[1.08505080e+11, 8.78742815e+10, 8.41016934e+10, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [5.59273120e+07, 4.95303320e+07, 4.81675920e+07, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [6.71369680e+07, 6.63744200e+07, 6.63236720e+07, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [4.86463120e+07, 4.77075920e+07, 3.71729280e+07, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [4.31885844e+10, 4.09234432e+10, 3.89777654e+10, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [8.78601900e+06, 6.83254900e+06, 6.57635900e+06, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]),\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_n_topics(n_topics: int):\n",
    "    model = TNTM_bow(\n",
    "        n_topics = n_topics,\n",
    "        save_path = \"C:\\\\Users\\\\arik_\\\\Documents\\\\Dokumente\\\\Job_Clausthal\\\\TNTM\\\\TNTM_Revision_TNNLS\\\\TNTM\\\\msc\\\\SavedResults\\\\model_v1.pth\",\n",
    "        n_dims = 11,\n",
    "        n_hidden_units = 200,\n",
    "        n_encoder_layers = 3,\n",
    "        enc_lr = 1e-4,\n",
    "        dec_lr = 1e-3,\n",
    "        n_epochs = 0,\n",
    "        #batch_size = 128,\n",
    "        batch_size = 256,\n",
    "        dropout_rate_encoder = 0.3,\n",
    "        prior_variance =  0.995, \n",
    "        prior_mean = None,\n",
    "        n_topwords = 200,\n",
    "        device = None, \n",
    "        validation_set_size = 0.2, \n",
    "        early_stopping = True,\n",
    "        n_epochs_early_stopping = 10,\n",
    "        return_embeddings = False,\n",
    "        eps = 1e-4,\n",
    "        umap_hyperparams = {'n_neighbors': 15, 'min_dist': 0.0},\n",
    "        return_losses= True\n",
    "    )\n",
    "\n",
    "    res_all = model.fit(\n",
    "        corpus = corpus, \n",
    "        vocab = vocab, \n",
    "        embeddings = embedding_ten\n",
    "    )\n",
    "\n",
    "    evaluation_result = score_all(\n",
    "        dataset = octis_dataset,\n",
    "        tw_emb=tw_emb,\n",
    "        n_words=10,\n",
    "        result = {'topics': res_all[0], \n",
    "                  \"topic-word-matrix\": res_all[1]},\n",
    "    )\n",
    "    \n",
    "    return evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate different number of topics\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_topics = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "n_iter = 10\n",
    "\n",
    "results = {}\n",
    "\n",
    "for m in range(n_iter):\n",
    "    for n in tqdm(n_topics):\n",
    "        results[n] = results.get(n, []) + [eval_model_n_topics(n)]\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "metrics = ['Embedding_Coherence', 'Topic Diversity', 'NPMI', \"Perplextiy\"]\n",
    "\n",
    "# make a boxplot for each metric and each number of topics\n",
    "\n",
    "for metric in metrics:\n",
    "    data = []\n",
    "    for n in n_topics:\n",
    "        data.append([res[metric] for res in results[n]])\n",
    "    plt.boxplot(data)\n",
    "    plt.title(metric)\n",
    "    plt.xticks(np.arange(len(n_topics))+1, n_topics)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# save results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TLDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
