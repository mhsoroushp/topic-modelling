{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/soroush/Partition 3/Topic-Modelling/.bertopicenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda\n",
      "current device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm \n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from Code.TNTM.TNTM_SentenceTransformer import TNTM_SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(41)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'current device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/Preprocessed_Data/octis_dataset_20ng.pickle\", \"rb\") as f: \n",
    "  dataset_raw_20ng = pickle.load(f)\n",
    "\n",
    "vocab = dataset_raw_20ng.get_vocabulary()  # alternative way of getting unique word list\n",
    "corpus = dataset_raw_20ng.get_corpus()    # list of documents as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3349"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique words\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totoal number of docs: 18846\n"
     ]
    }
   ],
   "source": [
    "# number of docs and each doc is converted as inner list of words\n",
    "print(\"totoal number of docs:\", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu> Subject: Pens fans reactions Organization: Post Office, Carnegie Mellon, Pittsburgh, PA Lines: 12 NNTP-Posting-Host: po4.andrew.cmu.edu    I am sure some bashers of Pens fans are pretty confused about the lack of any kind of posts about the recent Pens massacre of the Devils. Actually, I am  bit puzzled too and a bit relieved. However, I am going to put an end to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they are killing those Devils worse than I thought. Jagr just showed you why he is much better than his regular season stats. He is also a lot fo fun to watch in the playoffs. Bowman should let JAgr have a lot of fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final regular season game.          PENS RULE!!!  \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the original docs as sentence format\n",
    "with open(\"Data/Auxillary_Data/twng_textData.txt\", \"r\") as file:\n",
    "    data20ng_text = file.readlines()\n",
    "\n",
    "data20ng_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3349/3349 [00:10<00:00, 305.12it/s]\n",
      "/tmp/ipykernel_19682/1072508727.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  unique_words_embeddings = torch.Tensor(unique_words_embeddings)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3349, 384])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embeddings of unique words\n",
    "unique_words_embeddings = [embeddings_model.encode(word) for word in tqdm(vocab)]\n",
    "unique_words_embeddings = torch.Tensor(unique_words_embeddings)\n",
    "unique_words_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [01:31<00:00, 207.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([18846, 384])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create sentence embeddings\n",
    "# Load the model\n",
    "embeddings_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "sentence_embedding = []\n",
    "for line in tqdm(data20ng_text):\n",
    "    line_embedded =embeddings_model.encode(line.lower())\n",
    "    sentence_embedding.append(line_embedded)\n",
    "    \n",
    "sentence_embedding = torch.tensor(sentence_embedding)\n",
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_embedding = torch.randn(18846, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tntm = TNTM_SentenceTransformer(\n",
    "    n_topics = 20,\n",
    "    save_path = f\"Data/Auxillary_Data/{20}_topics\",\n",
    "    n_dims = 11,\n",
    "    n_hidden_units = 200,\n",
    "    n_encoder_layers = 2,\n",
    "    enc_lr = 1e-3,\n",
    "    dec_lr = 1e-3,\n",
    "    n_epochs = 20,\n",
    "    #batch_size = 128,\n",
    "    batch_size = 256,\n",
    "    dropout_rate_encoder = 0.3,\n",
    "    prior_variance =  0.995, \n",
    "    prior_mean = None,\n",
    "    n_topwords = 200,\n",
    "    device = device, \n",
    "    validation_set_size = 0.2, \n",
    "    early_stopping = True,\n",
    "    n_epochs_early_stopping = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TNTM_SentenceTransformer import TNTM_SentenceTransformer\n",
    "# tntm = TNTM_SentenceTransformer(\n",
    "#       n_topics  = 10, \n",
    "#       save_path = f\"Data/Auxillary_Data/{20}_topics\", \n",
    "#       enc_lr    = 1e-3,\n",
    "#       dec_lr    = 1e-3\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/soroush/Partition 3/Topic-Modelling/.bertopicenv/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12040. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/media/soroush/Partition 3/Topic-Modelling/TNTM/Code/TNTM/TNTM_SentenceTransformer.py:155: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mus_init_ten = torch.tensor(mus_init).to(self.device)\n",
      "/media/soroush/Partition 3/Topic-Modelling/TNTM/Code/TNTM/TNTM_SentenceTransformer.py:156: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  L_lower_init_ten = torch.tensor(L_lower_init).to(self.device)\n",
      "/media/soroush/Partition 3/Topic-Modelling/TNTM/Code/TNTM/TNTM_SentenceTransformer.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_diag_init_ten = torch.tensor(log_diag_init).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr 0: mean_train_loss = -3193.902099609375, mean_train_nl = -3216.8564453125, mean_train_kld = 22.954605102539062, elapsed time: 2.9492733478546143\n",
      "Epoch nr 0: median_train_loss = -3099.603515625, median_train_nl = -3119.6005859375, median_train_kld = 26.644495010375977, elapsed time: 2.9492733478546143\n",
      "Epoch nr 0: mean_val_loss = -3349.61669921875, mean_val_nl = -3377.052490234375, mean_val_kld = 27.43627166748047\n",
      "Epoch nr 0: median_val_loss = -3162.52197265625, median_val_nl = -3189.9443359375, median_val_kld = 27.44211196899414\n",
      "gradient norm: mean: 3580.283273556512, median: 2703.9600836174473, max: 14830.10748260662\n",
      "\n",
      "\n",
      "Epoch nr 1: mean_train_loss = -3384.67626953125, mean_train_nl = -3409.788818359375, mean_train_kld = 25.11225128173828, elapsed time: 2.793236255645752\n",
      "Epoch nr 1: median_train_loss = -3330.80078125, median_train_nl = -3356.02734375, median_train_kld = 25.349475860595703, elapsed time: 2.793236255645752\n",
      "Epoch nr 1: mean_val_loss = -3461.30078125, mean_val_nl = -3482.993896484375, mean_val_kld = 21.692827224731445\n",
      "Epoch nr 1: median_val_loss = -3522.797607421875, median_val_nl = -3544.56884765625, median_val_kld = 21.710681915283203\n",
      "gradient norm: mean: 1731.9260904190191, median: 1501.3433012477212, max: 5415.493867857555\n",
      "\n",
      "\n",
      "Epoch nr 2: mean_train_loss = -3485.986572265625, mean_train_nl = -3506.283203125, mean_train_kld = 20.29646873474121, elapsed time: 2.660529851913452\n",
      "Epoch nr 2: median_train_loss = -3342.952392578125, median_train_nl = -3363.94580078125, median_train_kld = 20.321392059326172, elapsed time: 2.660529851913452\n",
      "Epoch nr 2: mean_val_loss = -3558.5341796875, mean_val_nl = -3577.520263671875, mean_val_kld = 18.98644256591797\n",
      "Epoch nr 2: median_val_loss = -3495.746337890625, median_val_nl = -3514.804443359375, median_val_kld = 19.011089324951172\n",
      "gradient norm: mean: 1867.3332019808506, median: 1516.5109333931744, max: 7206.596193783782\n",
      "\n",
      "\n",
      "Epoch nr 3: mean_train_loss = -3557.381103515625, mean_train_nl = -3575.86328125, mean_train_kld = 18.48186492919922, elapsed time: 2.7604920864105225\n",
      "Epoch nr 3: median_train_loss = -3514.12353515625, median_train_nl = -3532.68359375, median_train_kld = 18.47674560546875, elapsed time: 2.7604920864105225\n",
      "Epoch nr 3: mean_val_loss = -3603.1201171875, mean_val_nl = -3621.70654296875, mean_val_kld = 18.586284637451172\n",
      "Epoch nr 3: median_val_loss = -3626.078369140625, median_val_nl = -3644.78271484375, median_val_kld = 18.629369735717773\n",
      "gradient norm: mean: 1952.4864505674677, median: 1725.464709064751, max: 7288.343651230641\n",
      "\n",
      "\n",
      "Epoch nr 4: mean_train_loss = -3597.158203125, mean_train_nl = -3615.927734375, mean_train_kld = 18.76981544494629, elapsed time: 2.7771990299224854\n",
      "Epoch nr 4: median_train_loss = -3494.341552734375, median_train_nl = -3513.05126953125, median_train_kld = 18.758939743041992, elapsed time: 2.7771990299224854\n",
      "Epoch nr 4: mean_val_loss = -3634.2216796875, mean_val_nl = -3653.47412109375, mean_val_kld = 19.25333023071289\n",
      "Epoch nr 4: median_val_loss = -3695.37255859375, median_val_nl = -3714.523681640625, median_val_kld = 19.242328643798828\n",
      "gradient norm: mean: 1941.1123535640218, median: 1753.8026600513344, max: 6190.967223883429\n",
      "\n",
      "\n",
      "Epoch nr 5: mean_train_loss = -3631.080810546875, mean_train_nl = -3650.525634765625, mean_train_kld = 19.445098876953125, elapsed time: 2.7205965518951416\n",
      "Epoch nr 5: median_train_loss = -3641.50390625, median_train_nl = -3661.25439453125, median_train_kld = 19.456289291381836, elapsed time: 2.7205965518951416\n",
      "Epoch nr 5: mean_val_loss = -3699.929931640625, mean_val_nl = -3718.8349609375, mean_val_kld = 18.904809951782227\n",
      "Epoch nr 5: median_val_loss = -3627.13671875, median_val_nl = -3646.24365234375, median_val_kld = 18.93667984008789\n",
      "gradient norm: mean: 2164.475342504207, median: 1934.5138238210955, max: 5755.466830330777\n",
      "\n",
      "\n",
      "Epoch nr 6: mean_train_loss = -3667.31982421875, mean_train_nl = -3685.59716796875, mean_train_kld = 18.277164459228516, elapsed time: 2.773850440979004\n",
      "Epoch nr 6: median_train_loss = -3542.40185546875, median_train_nl = -3560.599853515625, median_train_kld = 18.235733032226562, elapsed time: 2.773850440979004\n",
      "Epoch nr 6: mean_val_loss = -3699.2255859375, mean_val_nl = -3718.103271484375, mean_val_kld = 18.87818145751953\n",
      "Epoch nr 6: median_val_loss = -3683.705078125, median_val_nl = -3702.607421875, median_val_kld = 18.879074096679688\n",
      "gradient norm: mean: 2044.4545886512615, median: 1808.9194431519625, max: 4667.552866111962\n",
      "\n",
      "\n",
      "Epoch nr 7: mean_train_loss = -3697.871826171875, mean_train_nl = -3716.82373046875, mean_train_kld = 18.951763153076172, elapsed time: 2.7607223987579346\n",
      "Epoch nr 7: median_train_loss = -3515.784912109375, median_train_nl = -3534.638671875, median_train_kld = 18.960655212402344, elapsed time: 2.7607223987579346\n",
      "Epoch nr 7: mean_val_loss = -3731.395751953125, mean_val_nl = -3750.4189453125, mean_val_kld = 19.022886276245117\n",
      "Epoch nr 7: median_val_loss = -3627.669921875, median_val_nl = -3646.75244140625, median_val_kld = 19.001522064208984\n",
      "gradient norm: mean: 2179.2278476951, median: 1860.2688065552727, max: 6435.005495157819\n",
      "\n",
      "\n",
      "Epoch nr 8: mean_train_loss = -3728.94677734375, mean_train_nl = -3747.800537109375, mean_train_kld = 18.854019165039062, elapsed time: 2.7112908363342285\n",
      "Epoch nr 8: median_train_loss = -3721.3798828125, median_train_nl = -3740.385009765625, median_train_kld = 18.857759475708008, elapsed time: 2.7112908363342285\n",
      "Epoch nr 8: mean_val_loss = -3774.364990234375, mean_val_nl = -3793.034423828125, mean_val_kld = 18.669498443603516\n",
      "Epoch nr 8: median_val_loss = -3666.080810546875, median_val_nl = -3684.817626953125, median_val_kld = 18.6490478515625\n",
      "gradient norm: mean: 2443.346133359762, median: 2065.4155942950138, max: 10365.657131481947\n",
      "\n",
      "\n",
      "Epoch nr 9: mean_train_loss = -3755.86572265625, mean_train_nl = -3774.640869140625, mean_train_kld = 18.77552604675293, elapsed time: 2.786647081375122\n",
      "Epoch nr 9: median_train_loss = -3665.97314453125, median_train_nl = -3684.5927734375, median_train_kld = 18.794008255004883, elapsed time: 2.786647081375122\n",
      "Epoch nr 9: mean_val_loss = -3783.001708984375, mean_val_nl = -3801.99853515625, mean_val_kld = 18.99695587158203\n",
      "Epoch nr 9: median_val_loss = -3566.296630859375, median_val_nl = -3585.4736328125, median_val_kld = 18.974851608276367\n",
      "gradient norm: mean: 2726.1799251837274, median: 2328.414017100259, max: 11273.961062030743\n",
      "\n",
      "\n",
      "Epoch nr 10: mean_train_loss = -3784.546142578125, mean_train_nl = -3803.5029296875, mean_train_kld = 18.956846237182617, elapsed time: 2.7319529056549072\n",
      "Epoch nr 10: median_train_loss = -3755.08935546875, median_train_nl = -3773.92138671875, median_train_kld = 18.980974197387695, elapsed time: 2.7319529056549072\n",
      "Epoch nr 10: mean_val_loss = -3818.15478515625, mean_val_nl = -3836.746826171875, mean_val_kld = 18.59217071533203\n",
      "Epoch nr 10: median_val_loss = -3833.257568359375, median_val_nl = -3851.791259765625, median_val_kld = 18.617568969726562\n",
      "gradient norm: mean: 2450.7743390265723, median: 2168.75123400295, max: 8579.899351862816\n",
      "\n",
      "\n",
      "Epoch nr 11: mean_train_loss = -3808.77197265625, mean_train_nl = -3827.448974609375, mean_train_kld = 18.67662239074707, elapsed time: 2.8144452571868896\n",
      "Epoch nr 11: median_train_loss = -3739.44970703125, median_train_nl = -3758.333251953125, median_train_kld = 18.65205955505371, elapsed time: 2.8144452571868896\n",
      "Epoch nr 11: mean_val_loss = -3837.6494140625, mean_val_nl = -3856.828857421875, mean_val_kld = 19.179553985595703\n",
      "Epoch nr 11: median_val_loss = -3796.354248046875, median_val_nl = -3815.604736328125, median_val_kld = 19.172632217407227\n",
      "gradient norm: mean: 2758.4734053878924, median: 2554.553741718827, max: 7306.468954819806\n",
      "\n",
      "\n",
      "Epoch nr 12: mean_train_loss = -3829.48095703125, mean_train_nl = -3848.471435546875, mean_train_kld = 18.990619659423828, elapsed time: 2.7806355953216553\n",
      "Epoch nr 12: median_train_loss = -3716.504638671875, median_train_nl = -3735.83544921875, median_train_kld = 19.093807220458984, elapsed time: 2.7806355953216553\n",
      "Epoch nr 12: mean_val_loss = -3861.582275390625, mean_val_nl = -3879.9931640625, mean_val_kld = 18.410888671875\n",
      "Epoch nr 12: median_val_loss = -3835.42236328125, median_val_nl = -3853.916748046875, median_val_kld = 18.397340774536133\n",
      "gradient norm: mean: 2786.5484703868037, median: 2591.2780953225165, max: 6694.131838724755\n",
      "\n",
      "\n",
      "Epoch nr 13: mean_train_loss = -3853.456787109375, mean_train_nl = -3872.300537109375, mean_train_kld = 18.84385108947754, elapsed time: 2.844434976577759\n",
      "Epoch nr 13: median_train_loss = -3757.4794921875, median_train_nl = -3776.30224609375, median_train_kld = 18.94860076904297, elapsed time: 2.844434976577759\n",
      "Epoch nr 13: mean_val_loss = -3903.45849609375, mean_val_nl = -3922.84228515625, mean_val_kld = 19.383764266967773\n",
      "Epoch nr 13: median_val_loss = -3984.870849609375, median_val_nl = -4004.3759765625, median_val_kld = 19.397951126098633\n",
      "gradient norm: mean: 3062.9478686385846, median: 2673.3064575667154, max: 9319.57864007275\n",
      "\n",
      "\n",
      "Epoch nr 14: mean_train_loss = -3875.986328125, mean_train_nl = -3894.997802734375, mean_train_kld = 19.011030197143555, elapsed time: 2.863055944442749\n",
      "Epoch nr 14: median_train_loss = -3818.13818359375, median_train_nl = -3837.003173828125, median_train_kld = 19.00327491760254, elapsed time: 2.863055944442749\n",
      "Epoch nr 14: mean_val_loss = -3909.0185546875, mean_val_nl = -3927.665283203125, mean_val_kld = 18.646753311157227\n",
      "Epoch nr 14: median_val_loss = -3889.72705078125, median_val_nl = -3908.347900390625, median_val_kld = 18.6627140045166\n",
      "gradient norm: mean: 3268.6266657845067, median: 2815.3176235621627, max: 10606.893626017947\n",
      "\n",
      "\n",
      "Epoch nr 15: mean_train_loss = -3902.60009765625, mean_train_nl = -3920.9755859375, mean_train_kld = 18.375633239746094, elapsed time: 2.7916831970214844\n",
      "Epoch nr 15: median_train_loss = -3823.364501953125, median_train_nl = -3841.682861328125, median_train_kld = 18.31885528564453, elapsed time: 2.7916831970214844\n",
      "Epoch nr 15: mean_val_loss = -3929.5625, mean_val_nl = -3948.586669921875, mean_val_kld = 19.024450302124023\n",
      "Epoch nr 15: median_val_loss = -3925.924560546875, median_val_nl = -3944.99755859375, median_val_kld = 19.03091812133789\n",
      "gradient norm: mean: 3104.879276661875, median: 2822.1158134785655, max: 8317.897751458453\n",
      "\n",
      "\n",
      "Epoch nr 16: mean_train_loss = -3948.97705078125, mean_train_nl = -3967.65673828125, mean_train_kld = 18.6798152923584, elapsed time: 2.841171979904175\n",
      "Epoch nr 16: median_train_loss = -3861.642822265625, median_train_nl = -3879.862548828125, median_train_kld = 18.659944534301758, elapsed time: 2.841171979904175\n",
      "Epoch nr 16: mean_val_loss = -4001.96728515625, mean_val_nl = -4020.455322265625, mean_val_kld = 18.48825454711914\n",
      "Epoch nr 16: median_val_loss = -3914.03125, median_val_nl = -3932.411376953125, median_val_kld = 18.499839782714844\n",
      "gradient norm: mean: 3257.104257357475, median: 2974.3934697958643, max: 5947.519436697302\n",
      "\n",
      "\n",
      "Epoch nr 17: mean_train_loss = -4000.0908203125, mean_train_nl = -4019.228271484375, mean_train_kld = 19.13753318786621, elapsed time: 2.807777166366577\n",
      "Epoch nr 17: median_train_loss = -3908.82080078125, median_train_nl = -3927.40087890625, median_train_kld = 19.190874099731445, elapsed time: 2.807777166366577\n",
      "Epoch nr 17: mean_val_loss = -4032.11572265625, mean_val_nl = -4050.760009765625, mean_val_kld = 18.644338607788086\n",
      "Epoch nr 17: median_val_loss = -3944.523681640625, median_val_nl = -3963.01806640625, median_val_kld = 18.635845184326172\n",
      "gradient norm: mean: 3478.745026392366, median: 3073.663384577172, max: 9305.501881946884\n",
      "\n",
      "\n",
      "Epoch nr 18: mean_train_loss = -4021.675048828125, mean_train_nl = -4040.17578125, mean_train_kld = 18.5009765625, elapsed time: 2.8738455772399902\n",
      "Epoch nr 18: median_train_loss = -3996.336181640625, median_train_nl = -4014.783447265625, median_train_kld = 18.510160446166992, elapsed time: 2.8738455772399902\n",
      "Epoch nr 18: mean_val_loss = -4059.180908203125, mean_val_nl = -4077.7021484375, mean_val_kld = 18.52065086364746\n",
      "Epoch nr 18: median_val_loss = -3943.436767578125, median_val_nl = -3961.950439453125, median_val_kld = 18.518157958984375\n",
      "gradient norm: mean: 3986.483696678562, median: 3313.6433168452263, max: 10841.397295921415\n",
      "\n",
      "\n",
      "Epoch nr 19: mean_train_loss = -4044.338623046875, mean_train_nl = -4063.155517578125, mean_train_kld = 18.81686782836914, elapsed time: 2.76118540763855\n",
      "Epoch nr 19: median_train_loss = -4036.29052734375, median_train_nl = -4054.856201171875, median_train_kld = 18.833431243896484, elapsed time: 2.76118540763855\n",
      "Epoch nr 19: mean_val_loss = -4075.647216796875, mean_val_nl = -4094.2998046875, mean_val_kld = 18.65239906311035\n",
      "Epoch nr 19: median_val_loss = -4069.250732421875, median_val_nl = -4088.013671875, median_val_kld = 18.649398803710938\n",
      "gradient norm: mean: 3792.383215949973, median: 3329.497863367633, max: 10409.927150875908\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = tntm.fit(\n",
    "              corpus              = corpus,\n",
    "              vocab               = vocab, \n",
    "              word_embeddings     = unique_words_embeddings,\n",
    "              document_embeddings = sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "# result[0] is word distribution with shape #num_of_topics x num_unique_words\n",
    "# result[1] is weights corresponding to each word with shape #num_of_topics x num_unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = result[1]\n",
    "# normalize weights for each corresponding unique word\n",
    "normalize_weights = weights/weights.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: ['to', 'having', 'have', 'an', 'a', 'with', 'and', 'of', 'the', 'as']\n",
      "Cluster 2: ['sin', 'biblical', 'atheist', 'testament', 'orthodox', 'bible', 'atheism', 'christian', 'christianity', 'religion']\n",
      "Cluster 3: ['air', 'physics', 'motor', 'wind', 'sky', 'shuttle', 'train', 'rocket', 'engine', 'propulsion']\n",
      "Cluster 4: ['careful', 'take', 'save', 'go', 'proceed', 'control', 'away', 'block', 'flee', 'escape']\n",
      "Cluster 5: ['na', 'sp', 'pointer', 'pp', 'm', 'p', 'y', 'gw', 'l', 'f']\n",
      "Cluster 6: ['useless', 'care', 'medicine', 'infection', 'no', 'lack', 'neither', 'non', 'never', 'not']\n",
      "Cluster 7: ['most', 'these', 'etc', 'such', 'significantly', 'either', 'another', 'like', 'widely', 'some']\n",
      "Cluster 8: ['practical', 'participate', 'organize', 'capacity', 'integrate', 'qualify', 'incorporate', 'apply', 'accomplish', 'advanced']\n",
      "Cluster 9: ['jeff', 'randy', 'tom', 'james', 'george', 'walker', 'walter', 'brian', 'christopher', 'bob']\n",
      "Cluster 10: ['guy', 'anybody', 'person', 'friend', 'whom', 'somebody', 'someone', 'fellow', 'whose', 'whoever']\n",
      "Cluster 11: ['internet', 'ip', 'network', 'hardware', 'computer', 'device', 'windows', 'sony', 'floppy', 'load']\n",
      "Cluster 12: ['atlanta', 'iowa', 'au', 'state', 'alaska', 'australia', 'team', 'states', 'league', 'seattle']\n",
      "Cluster 13: ['concept', 'imagine', 'understanding', 'interpret', 'aspect', 'specify', 'regard', 'perspective', 'use', 'purpose']\n",
      "Cluster 14: ['teach', 'student', 'lesson', 'polytechnic', 'instruction', 'ucs', 'uchicago', 'college', 'university', 'edu']\n",
      "Cluster 15: ['when', 'morning', 'annual', 'month', 'august', 'spring', 'june', 'day', 'april', 'be']\n",
      "Cluster 16: ['location', 'surround', 'top', 'area', 'ground', 'center', 'where', 'building', 'place', 'spot']\n",
      "Cluster 17: ['officer', 'police', 'cop', 'guard', 'attack', 'penalty', 'offense', 'invade', 'punish', 'raid']\n",
      "Cluster 18: ['govt', 'business', 'minister', 'enterprise', 'agency', 'representative', 'administration', 'leader', 'organization', 'entity']\n",
      "Cluster 19: ['article', 'film', 'video', 'view', 'message', 'viewer', 'writing', 'letter', 'content', 'write']\n",
      "Cluster 20: ['probably', 'could', 'guess', 'whether', 'hopefully', 'can', 'maybe', 'presumably', 'would', 'if']\n"
     ]
    }
   ],
   "source": [
    "# Select top-k words for each cluster\n",
    "top_k = 10\n",
    "top_words_per_cluster = []\n",
    "for cluster_idx in range(normalize_weights.shape[0]):  # Iterate over clusters\n",
    "    # Get weights for all words in the cluster\n",
    "    word_weights = normalize_weights[cluster_idx]\n",
    "    \n",
    "    # Get indices of the top-k words\n",
    "    top_k_indices = word_weights.argsort()[-top_k:]\n",
    "    \n",
    "    # Map indices to words using resulttt[0]\n",
    "    top_words = [result[0][cluster_idx][i] for i in top_k_indices]\n",
    "    top_words_per_cluster.append(top_words)\n",
    "\n",
    "# Print the top-k words for each cluster\n",
    "for cluster_idx, words in enumerate(top_words_per_cluster):\n",
    "    print(f\"Cluster {cluster_idx + 1}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic \n",
    "### using BERTopic to assing one of the above topic to a document  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Embeddings Shape: (20, 384)\n"
     ]
    }
   ],
   "source": [
    "# compute weighted sum: weights_in_each_cluster x vocab_embeddings\n",
    "# np.dot(num_cluster x 3349, 3349 x embedding_dim) \n",
    "# output: num_cluster x embedding_dim\n",
    "vocab_embeddings = np.array(unique_words_embeddings)\n",
    "\n",
    "topic_embeddings = np.dot(normalize_weights, vocab_embeddings)\n",
    "print(f\"Topic Embeddings Shape: {topic_embeddings.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 is assigned to Topic 8\n",
      "Document 2 is assigned to Topic 19\n",
      "Document 3 is assigned to Topic 8\n",
      "Document 4 is assigned to Topic 8\n",
      "Document 5 is assigned to Topic 3\n",
      "Document 6 is assigned to Topic 2\n",
      "Document 7 is assigned to Topic 1\n",
      "Document 8 is assigned to Topic 19\n",
      "Document 9 is assigned to Topic 16\n",
      "Document 10 is assigned to Topic 20\n",
      "Document 11 is assigned to Topic 8\n",
      "Document 12 is assigned to Topic 8\n",
      "Document 13 is assigned to Topic 16\n",
      "Document 14 is assigned to Topic 8\n",
      "Document 15 is assigned to Topic 20\n",
      "Document 16 is assigned to Topic 8\n",
      "Document 17 is assigned to Topic 3\n",
      "Document 18 is assigned to Topic 5\n",
      "Document 19 is assigned to Topic 16\n",
      "Document 20 is assigned to Topic 4\n",
      "Document 21 is assigned to Topic 1\n",
      "Document 22 is assigned to Topic 8\n",
      "Document 23 is assigned to Topic 2\n",
      "Document 24 is assigned to Topic 10\n",
      "Document 25 is assigned to Topic 1\n",
      "Document 26 is assigned to Topic 8\n",
      "Document 27 is assigned to Topic 20\n",
      "Document 28 is assigned to Topic 8\n",
      "Document 29 is assigned to Topic 18\n",
      "Document 30 is assigned to Topic 20\n",
      "Document 31 is assigned to Topic 1\n",
      "Document 32 is assigned to Topic 16\n",
      "Document 33 is assigned to Topic 2\n",
      "Document 34 is assigned to Topic 8\n",
      "Document 35 is assigned to Topic 13\n",
      "Document 36 is assigned to Topic 16\n",
      "Document 37 is assigned to Topic 18\n",
      "Document 38 is assigned to Topic 16\n",
      "Document 39 is assigned to Topic 8\n",
      "Document 40 is assigned to Topic 16\n",
      "Document 41 is assigned to Topic 19\n",
      "Document 42 is assigned to Topic 16\n",
      "Document 43 is assigned to Topic 13\n",
      "Document 44 is assigned to Topic 8\n",
      "Document 45 is assigned to Topic 18\n",
      "Document 46 is assigned to Topic 2\n",
      "Document 47 is assigned to Topic 8\n",
      "Document 48 is assigned to Topic 16\n",
      "Document 49 is assigned to Topic 8\n",
      "Document 50 is assigned to Topic 3\n",
      "Document 51 is assigned to Topic 16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example: Document embeddings\n",
    "document_embeddings = np.array(sentence_embedding)  \n",
    "\n",
    "# Compute cosine similarity between document and topic embeddings\n",
    "similarity_matrix = cosine_similarity(document_embeddings, topic_embeddings)  # shape (50, 20)\n",
    "# Assign each document to the most similar topic\n",
    "document_topics = similarity_matrix.argmax(axis=1)  # len(corpus)\n",
    "\n",
    "# Print document-topic assignments\n",
    "for doc_idx, topic_idx in enumerate(document_topics):\n",
    "    print(f\"Document {doc_idx + 1} is assigned to Topic {topic_idx + 1}\")\n",
    "    if doc_idx == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since the lenght of top_words_per_cluster is not the same as corpus, here we create pseudo_document to be the same lenght as corpus\n",
    "pseudo_documents = [\" \".join(top_words_per_cluster[topic]) for topic in document_topics]\n",
    "len(pseudo_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use BERTopic to find the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic  Count                                          Name  \\\n",
      "0       -1   7816  -1_accomplish_incorporate_integrate_organize   \n",
      "1        0    545                0_place_center_location_ground   \n",
      "2        1    381                            1_not_non_never_no   \n",
      "3        2    302                         2_having_with_of_have   \n",
      "4        3    149                                 3_na_gw_pp_sp   \n",
      "..     ...    ...                                           ...   \n",
      "321    320     10          320_such_significantly_these_another   \n",
      "322    321     10               321_whom_whoever_anybody_fellow   \n",
      "323    322     10         322_accomplish_advanced_qualify_apply   \n",
      "324    323     10                        323_and_an_have_having   \n",
      "325    324     10                324_whose_whom_whoever_anybody   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [accomplish, incorporate, integrate, organize,...   \n",
      "1    [place, center, location, ground, area, where,...   \n",
      "2    [not, non, never, no, neither, medicine, lack,...   \n",
      "3    [having, with, of, have, and, an, as, to, the,...   \n",
      "4    [na, gw, pp, sp, pointer, integrate, capacity,...   \n",
      "..                                                 ...   \n",
      "321  [such, significantly, these, another, like, mo...   \n",
      "322  [whom, whoever, anybody, fellow, friend, someb...   \n",
      "323  [accomplish, advanced, qualify, apply, practic...   \n",
      "324  [and, an, have, having, as, of, the, to, with,...   \n",
      "325  [whose, whom, whoever, anybody, someone, someb...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [practical participate organize capacity integ...  \n",
      "1    [location surround top area ground center wher...  \n",
      "2    [useless care medicine infection no lack neith...  \n",
      "3    [sin biblical atheist testament orthodox bible...  \n",
      "4    [practical participate organize capacity integ...  \n",
      "..                                                 ...  \n",
      "321  [practical participate organize capacity integ...  \n",
      "322  [location surround top area ground center wher...  \n",
      "323  [practical participate organize capacity integ...  \n",
      "324  [to having have an a with and of the as, to ha...  \n",
      "325  [practical participate organize capacity integ...  \n",
      "\n",
      "[326 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# bert topic modelling\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Fit BERTopic with pseudo-documents and document embeddings\n",
    "topics, probs = topic_model.fit_transform(pseudo_documents, document_embeddings)\n",
    "\n",
    "# Display topics\n",
    "print(topic_model.get_topic_info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bertopicenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
