{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/soroush/Partition 3/temp-topic-modelling/.bertopicenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda\n",
      "current device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from tqdm import tqdm \n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "from Code.TNTM.TNTM_SentenceTransformer import TNTM_SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(41)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'current device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/DataResults_BERT/cleaned_embedding_df_20ng_BERT.pickle\", \"rb\") as f: \n",
    "  embedding_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.sort_values(by = \"word\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>[tensor(-0.4876), tensor(0.2411), tensor(0.158...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aa</th>\n",
       "      <td>[tensor(0.0616), tensor(1.8438), tensor(0.5808...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aaron</th>\n",
       "      <td>[tensor(-0.2090), tensor(0.5433), tensor(0.163...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ab</th>\n",
       "      <td>[tensor(-0.2713), tensor(0.9461), tensor(0.288...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ability</th>\n",
       "      <td>[tensor(-0.3597), tensor(0.2136), tensor(0.293...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeus</th>\n",
       "      <td>[tensor(-0.5143), tensor(1.4048), tensor(0.162...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zip</th>\n",
       "      <td>[tensor(0.6577), tensor(0.0648), tensor(0.9260...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zone</th>\n",
       "      <td>[tensor(-0.2998), tensor(-0.1618), tensor(0.38...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoo</th>\n",
       "      <td>[tensor(0.7611), tensor(1.6025), tensor(0.1737...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zuma</th>\n",
       "      <td>[tensor(-1.1792), tensor(0.2125), tensor(-0.24...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3349 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 embedding  is_valid\n",
       "word                                                                \n",
       "a        [tensor(-0.4876), tensor(0.2411), tensor(0.158...      True\n",
       "aa       [tensor(0.0616), tensor(1.8438), tensor(0.5808...      True\n",
       "aaron    [tensor(-0.2090), tensor(0.5433), tensor(0.163...      True\n",
       "ab       [tensor(-0.2713), tensor(0.9461), tensor(0.288...      True\n",
       "ability  [tensor(-0.3597), tensor(0.2136), tensor(0.293...      True\n",
       "...                                                    ...       ...\n",
       "zeus     [tensor(-0.5143), tensor(1.4048), tensor(0.162...      True\n",
       "zip      [tensor(0.6577), tensor(0.0648), tensor(0.9260...      True\n",
       "zone     [tensor(-0.2998), tensor(-0.1618), tensor(0.38...      True\n",
       "zoo      [tensor(0.7611), tensor(1.6025), tensor(0.1737...      True\n",
       "zuma     [tensor(-1.1792), tensor(0.2125), tensor(-0.24...      True\n",
       "\n",
       "[3349 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_ten_lis = []\n",
    "\n",
    "for i in range(len(embedding_df)):\n",
    "    embedding_ten_lis.append(embedding_df[\"embedding\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_ten = torch.stack(embedding_ten_lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3349, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_ten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/Preprocessed_Data/octis_dataset_20ng.pickle\", \"rb\") as f: \n",
    "  dataset_raw_20ng = pickle.load(f)\n",
    "\n",
    "vocab = dataset_raw_20ng.get_vocabulary()\n",
    "corpus = dataset_raw_20ng.get_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3349"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "embeddings_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"Data/Auxillary_Data/twng_textData.txt\", \"r\") as file:\n",
    "    data20ng_text = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu> Subject: Pens fans reactions Organization: Post Office, Carnegie Mellon, Pittsburgh, PA Lines: 12 NNTP-Posting-Host: po4.andrew.cmu.edu    I am sure some bashers of Pens fans are pretty confused about the lack of any kind of posts about the recent Pens massacre of the Devils. Actually, I am  bit puzzled too and a bit relieved. However, I am going to put an end to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they are killing those Devils worse than I thought. Jagr just showed you why he is much better than his regular season stats. He is also a lot fo fun to watch in the playoffs. Bowman should let JAgr have a lot of fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final regular season game.          PENS RULE!!!  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in data20ng_text:\n",
    "    print(line)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [01:30<00:00, 208.87it/s]\n",
      "/tmp/ipykernel_11568/4154606747.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  sentence_embedding = torch.tensor(sentence_embedding)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([18846, 384])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "sentence_embedding = []\n",
    "for line in tqdm(data20ng_text):\n",
    "    line_embedded =embeddings_model.encode(line.lower())\n",
    "    sentence_embedding.append(line_embedded)\n",
    "\n",
    "    \n",
    "sentence_embedding = torch.tensor(sentence_embedding)\n",
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_embedding = torch.randn(18846, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tntm = TNTM_SentenceTransformer(\n",
    "    n_topics = 10,\n",
    "    save_path = f\"Data/Auxillary_Data/{20}_topics\",\n",
    "    n_dims = 11,\n",
    "    n_hidden_units = 200,\n",
    "    n_encoder_layers = 2,\n",
    "    enc_lr = 1e-3,\n",
    "    dec_lr = 1e-3,\n",
    "    n_epochs = 20,\n",
    "    #batch_size = 128,\n",
    "    batch_size = 256,\n",
    "    dropout_rate_encoder = 0.3,\n",
    "    prior_variance =  0.995, \n",
    "    prior_mean = None,\n",
    "    n_topwords = 200,\n",
    "    device = device, \n",
    "    validation_set_size = 0.2, \n",
    "    early_stopping = True,\n",
    "    n_epochs_early_stopping = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TNTM_SentenceTransformer import TNTM_SentenceTransformer\n",
    "# tntm = TNTM_SentenceTransformer(\n",
    "#       n_topics  = 10, \n",
    "#       save_path = f\"example/{20}_topics\", \n",
    "#       enc_lr    = 1e-3,\n",
    "#       dec_lr    = 1e-3\n",
    "#       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/soroush/Partition 3/temp-topic-modelling/.bertopicenv/lib/python3.9/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12040. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/media/soroush/Partition 3/temp-topic-modelling/TNTM/Code/TNTM/TNTM_SentenceTransformer.py:155: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mus_init_ten = torch.tensor(mus_init).to(self.device)\n",
      "/media/soroush/Partition 3/temp-topic-modelling/TNTM/Code/TNTM/TNTM_SentenceTransformer.py:156: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  L_lower_init_ten = torch.tensor(L_lower_init).to(self.device)\n",
      "/media/soroush/Partition 3/temp-topic-modelling/TNTM/Code/TNTM/TNTM_SentenceTransformer.py:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  log_diag_init_ten = torch.tensor(log_diag_init).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr 0: mean_train_loss = -3769.306884765625, mean_train_nl = -3783.849853515625, mean_train_kld = 14.542951583862305, elapsed time: 1.7186377048492432\n",
      "Epoch nr 0: median_train_loss = -3751.2451171875, median_train_nl = -3754.071044921875, median_train_kld = 16.507986068725586, elapsed time: 1.7186377048492432\n",
      "Epoch nr 0: mean_val_loss = -3958.85791015625, mean_val_nl = -3975.9365234375, mean_val_kld = 17.078641891479492\n",
      "Epoch nr 0: median_val_loss = -3877.677490234375, median_val_nl = -3894.702880859375, median_val_kld = 17.08100128173828\n",
      "gradient norm: mean: 5297.024604291946, median: 3567.2001572752642, max: 19862.27348527945\n",
      "\n",
      "\n",
      "Epoch nr 1: mean_train_loss = -4008.54443359375, mean_train_nl = -4026.195068359375, mean_train_kld = 17.65085220336914, elapsed time: 1.775292158126831\n",
      "Epoch nr 1: median_train_loss = -3955.8876953125, median_train_nl = -3974.18115234375, median_train_kld = 17.687191009521484, elapsed time: 1.775292158126831\n",
      "Epoch nr 1: mean_val_loss = -4126.90771484375, mean_val_nl = -4143.48779296875, mean_val_kld = 16.580408096313477\n",
      "Epoch nr 1: median_val_loss = -4044.339599609375, median_val_nl = -4060.81494140625, median_val_kld = 16.581092834472656\n",
      "gradient norm: mean: 3281.1021067829324, median: 2471.562238976813, max: 16601.973747030726\n",
      "\n",
      "\n",
      "Epoch nr 2: mean_train_loss = -4121.6015625, mean_train_nl = -4137.60595703125, mean_train_kld = 16.0041561126709, elapsed time: 1.7432074546813965\n",
      "Epoch nr 2: median_train_loss = -4066.94921875, median_train_nl = -4083.25341796875, median_train_kld = 16.028793334960938, elapsed time: 1.7432074546813965\n",
      "Epoch nr 2: mean_val_loss = -4194.943359375, mean_val_nl = -4210.31982421875, mean_val_kld = 15.37623119354248\n",
      "Epoch nr 2: median_val_loss = -4108.4130859375, median_val_nl = -4123.6572265625, median_val_kld = 15.36737060546875\n",
      "gradient norm: mean: 3061.2504148690705, median: 2456.604744667986, max: 9922.573850156661\n",
      "\n",
      "\n",
      "Epoch nr 3: mean_train_loss = -4176.40380859375, mean_train_nl = -4190.0302734375, mean_train_kld = 13.626740455627441, elapsed time: 1.706700325012207\n",
      "Epoch nr 3: median_train_loss = -4134.0830078125, median_train_nl = -4148.873046875, median_train_kld = 13.453588485717773, elapsed time: 1.706700325012207\n",
      "Epoch nr 3: mean_val_loss = -4239.78759765625, mean_val_nl = -4252.09619140625, mean_val_kld = 12.308966636657715\n",
      "Epoch nr 3: median_val_loss = -4155.7548828125, median_val_nl = -4167.93310546875, median_val_kld = 12.302629470825195\n",
      "gradient norm: mean: 3304.896180886297, median: 2513.8680986093177, max: 18903.846830028262\n",
      "\n",
      "\n",
      "Epoch nr 4: mean_train_loss = -4211.93359375, mean_train_nl = -4224.119140625, mean_train_kld = 12.184965133666992, elapsed time: 1.688579797744751\n",
      "Epoch nr 4: median_train_loss = -4173.9462890625, median_train_nl = -4186.11376953125, median_train_kld = 12.173723220825195, elapsed time: 1.688579797744751\n",
      "Epoch nr 4: mean_val_loss = -4263.00732421875, mean_val_nl = -4275.39111328125, mean_val_kld = 12.383398056030273\n",
      "Epoch nr 4: median_val_loss = -4179.9814453125, median_val_nl = -4192.234375, median_val_kld = 12.39454460144043\n",
      "gradient norm: mean: 3310.973071931779, median: 2601.655229108569, max: 11454.198510529415\n",
      "\n",
      "\n",
      "Epoch nr 5: mean_train_loss = -4236.671875, mean_train_nl = -4248.98193359375, mean_train_kld = 12.309992790222168, elapsed time: 1.6694369316101074\n",
      "Epoch nr 5: median_train_loss = -4198.3994140625, median_train_nl = -4210.791015625, median_train_kld = 12.308293342590332, elapsed time: 1.6694369316101074\n",
      "Epoch nr 5: mean_val_loss = -4288.4033203125, mean_val_nl = -4300.71630859375, mean_val_kld = 12.312713623046875\n",
      "Epoch nr 5: median_val_loss = -4203.8759765625, median_val_nl = -4216.0703125, median_val_kld = 12.320823669433594\n",
      "gradient norm: mean: 3439.1958109228162, median: 2713.3379843762546, max: 15576.119116874426\n",
      "\n",
      "\n",
      "Epoch nr 6: mean_train_loss = -4263.62158203125, mean_train_nl = -4275.94287109375, mean_train_kld = 12.321101188659668, elapsed time: 1.7264845371246338\n",
      "Epoch nr 6: median_train_loss = -4226.88623046875, median_train_nl = -4239.20458984375, median_train_kld = 12.318384170532227, elapsed time: 1.7264845371246338\n",
      "Epoch nr 6: mean_val_loss = -4310.513671875, mean_val_nl = -4322.9580078125, mean_val_kld = 12.444738388061523\n",
      "Epoch nr 6: median_val_loss = -4225.1064453125, median_val_nl = -4237.44384765625, median_val_kld = 12.449304580688477\n",
      "gradient norm: mean: 3403.007811763653, median: 2828.769184859651, max: 10481.783826064291\n",
      "\n",
      "\n",
      "Epoch nr 7: mean_train_loss = -4283.3173828125, mean_train_nl = -4295.93603515625, mean_train_kld = 12.61882495880127, elapsed time: 1.6054389476776123\n",
      "Epoch nr 7: median_train_loss = -4247.81494140625, median_train_nl = -4260.5595703125, median_train_kld = 12.595970153808594, elapsed time: 1.6054389476776123\n",
      "Epoch nr 7: mean_val_loss = -4332.056640625, mean_val_nl = -4344.994140625, mean_val_kld = 12.936902046203613\n",
      "Epoch nr 7: median_val_loss = -4244.99072265625, median_val_nl = -4257.8408203125, median_val_kld = 12.950428009033203\n",
      "gradient norm: mean: 3654.8230615333487, median: 2957.1042438284303, max: 16167.05776092005\n",
      "\n",
      "\n",
      "Epoch nr 8: mean_train_loss = -4305.892578125, mean_train_nl = -4318.83984375, mean_train_kld = 12.947248458862305, elapsed time: 1.5845832824707031\n",
      "Epoch nr 8: median_train_loss = -4270.5634765625, median_train_nl = -4283.505859375, median_train_kld = 12.941625595092773, elapsed time: 1.5845832824707031\n",
      "Epoch nr 8: mean_val_loss = -4351.80322265625, mean_val_nl = -4364.845703125, mean_val_kld = 13.042329788208008\n",
      "Epoch nr 8: median_val_loss = -4265.39501953125, median_val_nl = -4278.3642578125, median_val_kld = 13.051210403442383\n",
      "gradient norm: mean: 3692.9122531048106, median: 3014.155841650966, max: 13472.355595018636\n",
      "\n",
      "\n",
      "Epoch nr 9: mean_train_loss = -4327.0390625, mean_train_nl = -4339.978515625, mean_train_kld = 12.938948631286621, elapsed time: 1.754889726638794\n",
      "Epoch nr 9: median_train_loss = -4287.3798828125, median_train_nl = -4300.1220703125, median_train_kld = 12.934886932373047, elapsed time: 1.754889726638794\n",
      "Epoch nr 9: mean_val_loss = -4371.6376953125, mean_val_nl = -4384.5234375, mean_val_kld = 12.88571548461914\n",
      "Epoch nr 9: median_val_loss = -4284.01416015625, median_val_nl = -4296.841796875, median_val_kld = 12.886740684509277\n",
      "gradient norm: mean: 3969.2349923577935, median: 3215.5262431097117, max: 15597.694361477397\n",
      "\n",
      "\n",
      "Epoch nr 10: mean_train_loss = -4348.23779296875, mean_train_nl = -4360.9951171875, mean_train_kld = 12.757746696472168, elapsed time: 1.5776903629302979\n",
      "Epoch nr 10: median_train_loss = -4309.8603515625, median_train_nl = -4322.3828125, median_train_kld = 12.776725769042969, elapsed time: 1.5776903629302979\n",
      "Epoch nr 10: mean_val_loss = -4390.3740234375, mean_val_nl = -4403.01611328125, mean_val_kld = 12.642498970031738\n",
      "Epoch nr 10: median_val_loss = -4304.03515625, median_val_nl = -4316.63427734375, median_val_kld = 12.64134407043457\n",
      "gradient norm: mean: 3994.493191973323, median: 3325.8041707550456, max: 13829.844628428462\n",
      "\n",
      "\n",
      "Epoch nr 11: mean_train_loss = -4368.12109375, mean_train_nl = -4380.50390625, mean_train_kld = 12.382620811462402, elapsed time: 1.569261074066162\n",
      "Epoch nr 11: median_train_loss = -4327.8359375, median_train_nl = -4339.9462890625, median_train_kld = 12.374963760375977, elapsed time: 1.569261074066162\n",
      "Epoch nr 11: mean_val_loss = -4410.30029296875, mean_val_nl = -4422.53466796875, mean_val_kld = 12.234978675842285\n",
      "Epoch nr 11: median_val_loss = -4322.2373046875, median_val_nl = -4334.43994140625, median_val_kld = 12.23633861541748\n",
      "gradient norm: mean: 4182.92703932317, median: 3411.1449704414913, max: 16001.347464311428\n",
      "\n",
      "\n",
      "Epoch nr 12: mean_train_loss = -4388.99267578125, mean_train_nl = -4401.2314453125, mean_train_kld = 12.239055633544922, elapsed time: 1.7463667392730713\n",
      "Epoch nr 12: median_train_loss = -4347.42333984375, median_train_nl = -4359.73876953125, median_train_kld = 12.22789192199707, elapsed time: 1.7463667392730713\n",
      "Epoch nr 12: mean_val_loss = -4428.06982421875, mean_val_nl = -4440.55224609375, mean_val_kld = 12.48207950592041\n",
      "Epoch nr 12: median_val_loss = -4341.8076171875, median_val_nl = -4354.26953125, median_val_kld = 12.48282241821289\n",
      "gradient norm: mean: 4142.479129757599, median: 3515.5082354729266, max: 12101.880429764009\n",
      "\n",
      "\n",
      "Epoch nr 13: mean_train_loss = -4405.3193359375, mean_train_nl = -4417.833984375, mean_train_kld = 12.514768600463867, elapsed time: 1.6378376483917236\n",
      "Epoch nr 13: median_train_loss = -4361.8173828125, median_train_nl = -4374.2744140625, median_train_kld = 12.509479522705078, elapsed time: 1.6378376483917236\n",
      "Epoch nr 13: mean_val_loss = -4448.5869140625, mean_val_nl = -4461.1904296875, mean_val_kld = 12.603766441345215\n",
      "Epoch nr 13: median_val_loss = -4362.671875, median_val_nl = -4375.2607421875, median_val_kld = 12.607468605041504\n",
      "gradient norm: mean: 4453.668854769131, median: 3709.212619205274, max: 17841.080520368796\n",
      "\n",
      "\n",
      "Epoch nr 14: mean_train_loss = -4426.5185546875, mean_train_nl = -4439.0673828125, mean_train_kld = 12.548954963684082, elapsed time: 1.7856121063232422\n",
      "Epoch nr 14: median_train_loss = -4381.7412109375, median_train_nl = -4394.11181640625, median_train_kld = 12.55484390258789, elapsed time: 1.7856121063232422\n",
      "Epoch nr 14: mean_val_loss = -4463.642578125, mean_val_nl = -4476.140625, mean_val_kld = 12.497509002685547\n",
      "Epoch nr 14: median_val_loss = -4378.9970703125, median_val_nl = -4391.48583984375, median_val_kld = 12.504566192626953\n",
      "gradient norm: mean: 4281.354698561664, median: 3868.443457365061, max: 11598.748005061907\n",
      "\n",
      "\n",
      "Epoch nr 15: mean_train_loss = -4443.673828125, mean_train_nl = -4456.0302734375, mean_train_kld = 12.356589317321777, elapsed time: 1.6040067672729492\n",
      "Epoch nr 15: median_train_loss = -4405.28662109375, median_train_nl = -4417.466796875, median_train_kld = 12.356081008911133, elapsed time: 1.6040067672729492\n",
      "Epoch nr 15: mean_val_loss = -4489.4326171875, mean_val_nl = -4501.748046875, mean_val_kld = 12.315506935119629\n",
      "Epoch nr 15: median_val_loss = -4402.51953125, median_val_nl = -4414.83056640625, median_val_kld = 12.331219673156738\n",
      "gradient norm: mean: 4781.134511069931, median: 3967.2671743891315, max: 19946.66128723338\n",
      "\n",
      "\n",
      "Epoch nr 16: mean_train_loss = -4469.24755859375, mean_train_nl = -4481.62646484375, mean_train_kld = 12.378917694091797, elapsed time: 1.6887280941009521\n",
      "Epoch nr 16: median_train_loss = -4428.6201171875, median_train_nl = -4441.064453125, median_train_kld = 12.36701774597168, elapsed time: 1.6887280941009521\n",
      "Epoch nr 16: mean_val_loss = -4512.5830078125, mean_val_nl = -4525.18310546875, mean_val_kld = 12.600796699523926\n",
      "Epoch nr 16: median_val_loss = -4424.974609375, median_val_nl = -4437.578125, median_val_kld = 12.620468139648438\n",
      "gradient norm: mean: 4794.072023559583, median: 4195.3243250819005, max: 14216.349751699112\n",
      "\n",
      "\n",
      "Epoch nr 17: mean_train_loss = -4489.103515625, mean_train_nl = -4501.59228515625, mean_train_kld = 12.48873233795166, elapsed time: 1.7861595153808594\n",
      "Epoch nr 17: median_train_loss = -4446.509765625, median_train_nl = -4458.7353515625, median_train_kld = 12.509593963623047, elapsed time: 1.7861595153808594\n",
      "Epoch nr 17: mean_val_loss = -4528.859375, mean_val_nl = -4541.2021484375, mean_val_kld = 12.343008041381836\n",
      "Epoch nr 17: median_val_loss = -4440.365234375, median_val_nl = -4452.7177734375, median_val_kld = 12.364070892333984\n",
      "gradient norm: mean: 4714.339575390732, median: 4184.056689250485, max: 12691.679221239261\n",
      "\n",
      "\n",
      "Epoch nr 18: mean_train_loss = -4502.14453125, mean_train_nl = -4514.474609375, mean_train_kld = 12.330117225646973, elapsed time: 1.622032880783081\n",
      "Epoch nr 18: median_train_loss = -4456.8154296875, median_train_nl = -4469.21728515625, median_train_kld = 12.329605102539062, elapsed time: 1.622032880783081\n",
      "Epoch nr 18: mean_val_loss = -4539.3046875, mean_val_nl = -4551.8828125, mean_val_kld = 12.57777214050293\n",
      "Epoch nr 18: median_val_loss = -4451.3515625, median_val_nl = -4463.951171875, median_val_kld = 12.60006332397461\n",
      "gradient norm: mean: 4973.051389601567, median: 4335.063104422327, max: 15938.74732941211\n",
      "\n",
      "\n",
      "Epoch nr 19: mean_train_loss = -4516.95263671875, mean_train_nl = -4529.55712890625, mean_train_kld = 12.604903221130371, elapsed time: 1.6174674034118652\n",
      "Epoch nr 19: median_train_loss = -4469.54833984375, median_train_nl = -4482.04345703125, median_train_kld = 12.599594116210938, elapsed time: 1.6174674034118652\n",
      "Epoch nr 19: mean_val_loss = -4556.3427734375, mean_val_nl = -4568.96142578125, mean_val_kld = 12.618953704833984\n",
      "Epoch nr 19: median_val_loss = -4466.33251953125, median_val_nl = -4478.98046875, median_val_kld = 12.644258499145508\n",
      "gradient norm: mean: 5138.0738643909235, median: 4595.357460524708, max: 17545.874562083933\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = tntm.fit(\n",
    "              corpus              = corpus,\n",
    "              vocab               = vocab, \n",
    "              word_embeddings     = embedding_ten,\n",
    "              document_embeddings = sentence_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = result[1]\n",
    "# normalize weights for each corresponding unique word\n",
    "normalize_weights = weights/weights.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1: hi (Score: 0.0309)\n",
      "Cluster 2: focus (Score: 0.0241)\n",
      "Cluster 3: more (Score: 0.0199)\n",
      "Cluster 4: ground (Score: 0.0331)\n",
      "Cluster 5: the (Score: 0.1173)\n",
      "Cluster 6: gw (Score: 0.0453)\n",
      "Cluster 7: nation (Score: 0.0333)\n",
      "Cluster 8: address (Score: 0.0272)\n",
      "Cluster 9: ted (Score: 0.1018)\n",
      "Cluster 10: go (Score: 0.0867)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "num_clusters = result[1].shape[0]  # Number of clusters\n",
    "num_words = result[1].shape[1]  # Number of unique words\n",
    "\n",
    "# Extract the most relevant word and its score for each cluster\n",
    "most_relevant_words = []\n",
    "\n",
    "for cluster_idx in range(num_clusters):\n",
    "    # Get the weight scores for all words in the current cluster\n",
    "    word_scores = normalize_weights[cluster_idx]\n",
    "    \n",
    "    most_relevant_word_idx = word_scores.argmax()\n",
    "    \n",
    "    # Retrieve the actual word using result[0] (the vocabulary)\n",
    "    most_relevant_word = result[0][cluster_idx][most_relevant_word_idx]\n",
    "    \n",
    "    most_relevant_score = word_scores[most_relevant_word_idx]\n",
    "    \n",
    "    most_relevant_words.append((most_relevant_word, most_relevant_score))\n",
    "\n",
    "for cluster_idx, (word, score) in enumerate(most_relevant_words):\n",
    "    print(f\"Cluster {cluster_idx + 1}: {word} (Score: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTopic \n",
    "### using BERTopic to assing one of the above topic to a document  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".bertopicenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
